{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting = 3)\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300 \n",
    "min_word_count = 40 \n",
    "num_workers = 4\n",
    "context = 10 \n",
    "downsampling = 1e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    review_text = BeautifulSoup(review,\"lxml\").get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16490, 300)\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.06587438e-02 -1.30716218e-02 -2.69954707e-02  6.33645728e-02\n",
      " -6.51245043e-02 -6.32605106e-02 -4.81012336e-04 -1.11888098e-02\n",
      " -7.76432529e-02 -4.86407056e-02  9.42130908e-02  5.38372397e-02\n",
      "  5.48828393e-02  7.37902569e-03 -1.48566850e-02  2.60501076e-02\n",
      " -5.07104732e-02  1.32804230e-01  2.36674372e-04 -6.28971010e-02\n",
      "  5.41811902e-03  1.13381026e-02 -9.77564007e-02 -2.56607942e-02\n",
      " -4.20020781e-02  6.11054152e-02  7.67687634e-02  2.49413797e-03\n",
      "  2.04210775e-03 -1.03719108e-01  1.49613708e-01  4.36168276e-02\n",
      "  4.21950445e-02  6.13381118e-02 -3.76603231e-02 -2.22360268e-02\n",
      " -6.94532022e-02 -1.18287876e-01  3.16194966e-02  6.24483339e-02\n",
      " -7.27119520e-02  4.38906811e-02  3.55872139e-02 -7.94034302e-02\n",
      "  5.18171936e-02 -4.87119611e-03  6.46155477e-02  5.70900179e-02\n",
      " -9.54613760e-02 -3.84350047e-02  6.64945915e-02  6.05421513e-03\n",
      " -2.43431292e-02  9.96646360e-02 -2.78943069e-02  1.27462600e-03\n",
      " -7.82098528e-03  1.18875699e-02  2.63399892e-02  3.64927538e-02\n",
      "  1.58986285e-01  4.92288321e-02  6.50738031e-02 -6.60474896e-02\n",
      "  1.69882346e-02  1.09965429e-01 -1.16702825e-01 -2.16027535e-02\n",
      "  1.81027893e-02 -3.29287648e-02 -8.30798000e-02 -2.18720604e-02\n",
      "  1.25727896e-02  1.97272766e-02 -3.44521180e-02 -3.05009075e-03\n",
      "  4.84457798e-02  2.43155863e-02  4.04503681e-02 -7.21980333e-02\n",
      " -1.08213775e-01  5.18593006e-02 -2.08890904e-03  3.38584557e-02\n",
      " -5.82543388e-02  4.19011675e-02 -1.16736010e-01 -1.11204281e-03\n",
      " -4.89902012e-02  2.49638967e-02 -2.61373799e-02  1.91958174e-02\n",
      "  8.25976729e-02 -3.49186510e-02 -8.09670165e-02  9.04806182e-02\n",
      "  2.77277324e-02  1.02005713e-02 -4.61018085e-03 -9.26021487e-02\n",
      " -4.49943878e-02  1.02745360e-02  2.64197611e-03  6.46640286e-02\n",
      " -6.89030960e-02  6.21736757e-02  7.53861219e-02  6.83733746e-02\n",
      "  3.53292935e-02 -5.43386750e-02 -3.40784565e-02  1.89325530e-02\n",
      "  1.09615132e-01 -3.24382633e-02 -3.86222824e-02 -3.11763622e-02\n",
      " -4.59643006e-02  5.37033565e-02 -8.64544325e-03 -3.66534740e-02\n",
      " -8.01158324e-02  1.09802708e-02  2.95006600e-03 -4.44566719e-02\n",
      "  9.39921215e-02 -6.83037937e-02 -3.19231972e-02 -6.23443201e-02\n",
      " -2.54366994e-02 -7.36578479e-02  3.92726026e-02 -3.38134393e-02\n",
      "  3.39481118e-03 -2.38033570e-02 -8.98585990e-02 -7.03724101e-02\n",
      "  1.96036082e-02 -2.80378591e-02 -9.03380215e-02  1.14756167e-01\n",
      " -3.96956429e-02 -4.24554162e-02  1.90918222e-02  8.96954536e-03\n",
      "  3.17758787e-03 -2.09671166e-02  6.08861223e-02 -4.54355143e-02\n",
      " -2.12316513e-02 -1.26027212e-01 -2.41857325e-03  1.84959993e-02\n",
      " -5.89330681e-02 -4.89725545e-03  2.97843851e-02  2.90206969e-02\n",
      " -9.38193053e-02 -3.30552571e-02 -4.87569682e-02 -4.78586517e-02\n",
      " -7.18630254e-02 -1.17829435e-01 -5.22440523e-02  5.40046729e-02\n",
      " -9.92097892e-03 -3.39130461e-02 -2.06250716e-02  3.20742428e-02\n",
      "  1.16943635e-01 -1.16951400e-02 -1.69355739e-02 -7.22275674e-02\n",
      " -6.55029491e-02 -5.72157688e-02  3.76533531e-02 -3.04774288e-02\n",
      " -3.90419848e-02  3.45661268e-02 -9.88299325e-02  1.28033711e-02\n",
      "  7.56283626e-02 -2.29843054e-03  2.60496195e-02 -3.41099650e-02\n",
      "  1.01948986e-02  1.76547952e-02  2.58101299e-02 -1.28056686e-02\n",
      "  1.21638803e-02 -1.24278888e-02 -7.25601390e-02  6.99680485e-03\n",
      "  1.34709617e-03  4.41971645e-02 -2.83388738e-02  8.30738544e-02\n",
      "  5.91392145e-02 -3.27227674e-02  6.11889921e-02 -9.52204540e-02\n",
      "  8.31223577e-02 -2.60207858e-02  5.94093949e-02  7.48727396e-02\n",
      "  1.27948262e-02  7.43540563e-03 -7.48638213e-02 -4.43668775e-02\n",
      " -4.99170758e-02  8.35013092e-02 -4.57390631e-03 -3.60963121e-02\n",
      "  7.38280937e-02 -8.83278325e-02  4.78269486e-03 -2.29061209e-02\n",
      " -8.13220963e-02  6.79711550e-02  5.26113482e-03 -2.83216368e-02\n",
      " -8.30847025e-02  1.06940670e-02 -3.20375003e-02  2.82480456e-02\n",
      " -4.89145517e-02  3.43277790e-02  1.51697714e-02  1.74148865e-02\n",
      " -2.96414718e-02  6.93771392e-02  1.08507751e-02  4.04588282e-02\n",
      " -4.01405729e-02  1.10775037e-02  3.09955925e-02  1.21891778e-02\n",
      "  1.35109991e-01  4.27235998e-02 -8.63357354e-03  1.07994713e-02\n",
      " -9.76861790e-02  6.37627542e-02  5.21047823e-02 -1.51627166e-02\n",
      "  1.49275064e-02 -1.16791874e-04 -6.60610422e-02 -5.07792877e-03\n",
      "  6.79676980e-03  2.68857181e-03  7.30423331e-02  5.44125549e-02\n",
      "  3.84629108e-02 -3.59205939e-02 -3.71448435e-02 -1.46458656e-01\n",
      "  2.11552065e-02  1.01540096e-01  7.52742887e-02 -7.68654794e-02\n",
      "  7.37887574e-03 -2.36594900e-02 -1.18148409e-01  4.43922058e-02\n",
      " -8.65789875e-02  1.54347152e-01  9.37615782e-02 -1.02334410e-01\n",
      "  1.94583852e-02  5.99705838e-02  9.92467925e-02 -2.99560502e-02\n",
      " -5.89839034e-02 -6.61878362e-02 -5.98921953e-03 -3.10924686e-02\n",
      " -4.90227751e-02 -4.61697020e-02  4.25479710e-02 -5.68930097e-02\n",
      "  2.36191768e-02 -5.64559251e-02  4.13795300e-02 -4.02392484e-02\n",
      "  5.68962917e-02 -8.23816936e-03 -4.66268882e-02  1.09996106e-02\n",
      "  5.72368950e-02  1.02711111e-01 -9.73222032e-02 -1.16738481e-02\n",
      " -9.05620027e-03 -1.53643146e-01 -5.76941632e-02 -1.71019102e-03\n",
      "  9.95061547e-03 -1.01797849e-01 -3.41934301e-02  3.77753973e-02]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv[\"flower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords+1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    \n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    \n",
    "    counter = 0\n",
    "    counter = int(counter)\n",
    "    \n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    \n",
    "    for review in reviews:\n",
    "        if counter%1000. == 0.:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "        \n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for Training reviews\n",
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating average feature vecs for Training reviews\")\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "trainDataVecs = Imputer().fit_transform(trainDataVecs)\n",
    "\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataVecs = Imputer().fit_transform(testDataVecs)\n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "start = time.time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv.vectors\n",
    "num_clusters = int(word_vectors.shape[0] / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  1180.7468872070312 seconds.\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['bent']\n",
      "\n",
      "Cluster 1\n",
      "['gleefully', 'glee', 'vomiting', 'leering', 'teasing', 'orville']\n",
      "\n",
      "Cluster 2\n",
      "['olivia', 'havilland']\n",
      "\n",
      "Cluster 3\n",
      "['gunslinger', 'scrimm', 'lawman', 'wily']\n",
      "\n",
      "Cluster 4\n",
      "['diversity']\n",
      "\n",
      "Cluster 5\n",
      "['backs', 'throats', 'butts', 'noses', 'beds', 'pockets', 'ropes', 'tongues', 'necks']\n",
      "\n",
      "Cluster 6\n",
      "['bond', 'stewart', 'cagney', 'cameron', 'mason', 'garner', 'belushi', 'coburn', 'gleason', 'caan', 'russo', 'gandolfini', 'brolin', 'toback', 'spader', 'caviezel', 'duval', 'howe']\n",
      "\n",
      "Cluster 7\n",
      "['romania', 'denmark', 'netherlands', 'serbia']\n",
      "\n",
      "Cluster 8\n",
      "['maintains', 'maintained', 'entertains', 'retains', 'balances']\n",
      "\n",
      "Cluster 9\n",
      "['hopkins', 'dern', 'perkins', 'viggo', 'mortensen', 'elisha', 'christensen']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0,10):\n",
    "    \n",
    "    print(\"\\nCluster %d\" % cluster)\n",
    "   \n",
    "    words = []\n",
    "    for i in range(0,len(list(word_centroid_map.values()))):\n",
    "        if( list(word_centroid_map.values())[i] == cluster ):\n",
    "            words.append(list(word_centroid_map)[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
